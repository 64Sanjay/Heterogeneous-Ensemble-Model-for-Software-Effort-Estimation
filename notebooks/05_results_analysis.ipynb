{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Results Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of experimental results.\n",
    "\n",
    "## Contents:\n",
    "1. Run complete experiment\n",
    "2. Individual vs Ensemble comparison\n",
    "3. Statistical analysis\n",
    "4. Visualization\n",
    "5. Final conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.preprocessor import DataPreprocessor\n",
    "from src.models.ensemble_model import EnsembleModel\n",
    "from src.models.cbr_model import CBRModel\n",
    "from src.models.cocomo_model import COCOMOModel\n",
    "from src.models.ml_models import XGBoostModel, ANNModel, KNNModel, SVRModel\n",
    "from src.evaluation.metrics import calculate_all_metrics\n",
    "from src.evaluation.cross_validation import CrossValidator\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = DataLoader('cocomo81')\n",
    "df = loader.load_raw_data()\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "X, y = preprocessor.preprocess_pipeline(df, scale=True)\n",
    "\n",
    "print(f\"Dataset: COCOMO81\")\n",
    "print(f\"Samples: {len(y)}\")\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation\n",
    "cv = CrossValidator(cv_type='kfold', n_splits=5)\n",
    "\n",
    "print(\"Running 5-Fold Cross-Validation...\")\n",
    "print(\"This may take a few minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Individual Models\n",
    "individual_models = {\n",
    "    'CBR': CBRModel(),\n",
    "    'COCOMO': COCOMOModel(use_nn_correction=False),\n",
    "    'XGBoost': XGBoostModel(),\n",
    "    'ANN': ANNModel(),\n",
    "    'KNN': KNNModel(),\n",
    "    'SVR': SVRModel()\n",
    "}\n",
    "\n",
    "individual_results = []\n",
    "\n",
    "for name, model in individual_models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    result = cv.evaluate_model(model, X, y)\n",
    "    \n",
    "    individual_results.append({\n",
    "        'Model': name,\n",
    "        'MAE': result['metrics']['MAE'],\n",
    "        'RMSE': result['metrics']['RMSE'],\n",
    "        'MMRE': result['metrics']['MMRE'],\n",
    "        'MdMRE': result['metrics']['MdMRE'],\n",
    "        'PRED(0.25)': result['metrics']['PRED(0.25)'],\n",
    "        'Training Time': result['metrics']['Training_Time']\n",
    "    })\n",
    "\n",
    "individual_df = pd.DataFrame(individual_results)\n",
    "print(\"\\n✓ Individual models evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ensemble Models\n",
    "ml_variants = ['XGBoost', 'ANN', 'KNN', 'SVR']\n",
    "ensemble_results = []\n",
    "\n",
    "for ml_name in ml_variants:\n",
    "    print(f\"Evaluating Ensemble with {ml_name}...\")\n",
    "    \n",
    "    ensemble = EnsembleModel(ml_model_name=ml_name, combination_rule='median')\n",
    "    result = cv.evaluate_model(ensemble, X, y)\n",
    "    \n",
    "    ensemble_results.append({\n",
    "        'Model': f'CBR+COCOMO+{ml_name}',\n",
    "        'MAE': result['metrics']['MAE'],\n",
    "        'RMSE': result['metrics']['RMSE'],\n",
    "        'MMRE': result['metrics']['MMRE'],\n",
    "        'MdMRE': result['metrics']['MdMRE'],\n",
    "        'PRED(0.25)': result['metrics']['PRED(0.25)'],\n",
    "        'Training Time': result['metrics']['Training_Time']\n",
    "    })\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_results)\n",
    "print(\"\\n✓ Ensemble models evaluated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INDIVIDUAL MODELS RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(individual_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ENSEMBLE MODELS RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(ensemble_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models\n",
    "best_individual_idx = individual_df['MAE'].idxmin()\n",
    "best_individual = individual_df.loc[best_individual_idx]\n",
    "\n",
    "best_ensemble_idx = ensemble_df['MAE'].idxmin()\n",
    "best_ensemble = ensemble_df.loc[best_ensemble_idx]\n",
    "\n",
    "print(\"BEST MODELS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest Individual Model: {best_individual['Model']}\")\n",
    "print(f\"  MAE: {best_individual['MAE']:.2f}\")\n",
    "print(f\"  MMRE: {best_individual['MMRE']:.4f}\")\n",
    "print(f\"  PRED(0.25): {best_individual['PRED(0.25)']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Ensemble Model: {best_ensemble['Model']}\")\n",
    "print(f\"  MAE: {best_ensemble['MAE']:.2f}\")\n",
    "print(f\"  MMRE: {best_ensemble['MMRE']:.4f}\")\n",
    "print(f\"  PRED(0.25): {best_ensemble['PRED(0.25)']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement\n",
    "mae_improvement = ((best_individual['MAE'] - best_ensemble['MAE']) / best_individual['MAE']) * 100\n",
    "mmre_improvement = ((best_individual['MMRE'] - best_ensemble['MMRE']) / best_individual['MMRE']) * 100\n",
    "pred_improvement = ((best_ensemble['PRED(0.25)'] - best_individual['PRED(0.25)']) / best_individual['PRED(0.25)']) * 100\n",
    "\n",
    "print(\"\\nIMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "if mae_improvement > 0:\n",
    "    print(f\"MAE Improvement: ↓ {mae_improvement:.2f}% (Ensemble better)\")\n",
    "else:\n",
    "    print(f\"MAE Difference: ↑ {-mae_improvement:.2f}% (Individual better)\")\n",
    "\n",
    "if mmre_improvement > 0:\n",
    "    print(f\"MMRE Improvement: ↓ {mmre_improvement:.2f}% (Ensemble better)\")\n",
    "else:\n",
    "    print(f\"MMRE Difference: ↑ {-mmre_improvement:.2f}% (Individual better)\")\n",
    "\n",
    "if pred_improvement > 0:\n",
    "    print(f\"PRED(0.25) Improvement: ↑ {pred_improvement:.2f}% (Ensemble better)\")\n",
    "else:\n",
    "    print(f\"PRED(0.25) Difference: ↓ {-pred_improvement:.2f}% (Individual better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined comparison chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Prepare data\n",
    "all_models = pd.concat([\n",
    "    individual_df[['Model', 'MAE', 'MMRE', 'PRED(0.25)']].assign(Type='Individual'),\n",
    "    ensemble_df[['Model', 'MAE', 'MMRE', 'PRED(0.25)']].assign(Type='Ensemble')\n",
    "])\n",
    "\n",
    "colors = {'Individual': '#3498db', 'Ensemble': '#e74c3c'}\n",
    "\n",
    "# MAE Chart\n",
    "ax1 = axes[0]\n",
    "x = range(len(all_models))\n",
    "bars = ax1.bar(x, all_models['MAE'], color=[colors[t] for t in all_models['Type']])\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(all_models['Model'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.set_title('Mean Absolute Error (Lower is Better)')\n",
    "ax1.axhline(y=best_ensemble['MAE'], color='red', linestyle='--', alpha=0.7, label='Best Ensemble')\n",
    "\n",
    "# MMRE Chart\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(x, all_models['MMRE'], color=[colors[t] for t in all_models['Type']])\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(all_models['Model'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('MMRE')\n",
    "ax2.set_title('Mean Magnitude of Relative Error (Lower is Better)')\n",
    "\n",
    "# PRED Chart\n",
    "ax3 = axes[2]\n",
    "bars = ax3.bar(x, all_models['PRED(0.25)'], color=[colors[t] for t in all_models['Type']])\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(all_models['Model'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('PRED(0.25)')\n",
    "ax3.set_title('Prediction Accuracy within 25% (Higher is Better)')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors['Individual'], label='Individual'),\n",
    "                   Patch(facecolor=colors['Ensemble'], label='Ensemble')]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/final_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "all_times = pd.concat([\n",
    "    individual_df[['Model', 'Training Time']].assign(Type='Individual'),\n",
    "    ensemble_df[['Model', 'Training Time']].assign(Type='Ensemble')\n",
    "])\n",
    "\n",
    "x = range(len(all_times))\n",
    "bars = ax.bar(x, all_times['Training Time'], color=[colors[t] for t in all_times['Type']])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_times['Model'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Training Time (seconds)')\n",
    "ax.set_title('Model Training Time Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/training_time.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of all metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual models heatmap\n",
    "individual_metrics = individual_df.set_index('Model')[['MAE', 'RMSE', 'MMRE', 'MdMRE', 'PRED(0.25)']]\n",
    "# Normalize for visualization\n",
    "individual_norm = (individual_metrics - individual_metrics.min()) / (individual_metrics.max() - individual_metrics.min())\n",
    "\n",
    "sns.heatmap(individual_norm, annot=individual_metrics.round(3), fmt='', cmap='RdYlGn_r', ax=axes[0])\n",
    "axes[0].set_title('Individual Models Metrics')\n",
    "\n",
    "# Ensemble models heatmap\n",
    "ensemble_metrics = ensemble_df.set_index('Model')[['MAE', 'RMSE', 'MMRE', 'MdMRE', 'PRED(0.25)']]\n",
    "ensemble_norm = (ensemble_metrics - ensemble_metrics.min()) / (ensemble_metrics.max() - ensemble_metrics.min())\n",
    "\n",
    "sns.heatmap(ensemble_norm, annot=ensemble_metrics.round(3), fmt='', cmap='RdYlGn_r', ax=axes[1])\n",
    "axes[1].set_title('Ensemble Models Metrics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/metrics_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "output_file = '../experiments/results/cocomo81_final_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    individual_df.to_excel(writer, sheet_name='Individual_Models', index=False)\n",
    "    ensemble_df.to_excel(writer, sheet_name='Ensemble_Models', index=False)\n",
    "    \n",
    "    # Summary sheet\n",
    "    summary_data = {\n",
    "        'Metric': ['Best Individual Model', 'Best Individual MAE', 'Best Ensemble Model', 'Best Ensemble MAE', 'MAE Improvement (%)'],\n",
    "        'Value': [best_individual['Model'], best_individual['MAE'], best_ensemble['Model'], best_ensemble['MAE'], mae_improvement]\n",
    "    }\n",
    "    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Heterogeneous Ensemble Effectiveness**: The combination of CBR, COCOMO-II, and ML models provides robust predictions by leveraging diverse estimation approaches.\n",
    "\n",
    "2. **Best Configuration**: CBR + COCOMO + XGBoost with median combination rule typically provides the best balance of accuracy and computational efficiency.\n",
    "\n",
    "3. **Combination Rule**: Median combination is more robust than linear or mean combinations as it handles outlier predictions better.\n",
    "\n",
    "4. **Practical Implications**:\n",
    "   - Ensemble models reduce the risk of poor estimates from any single model\n",
    "   - The approach is applicable to different datasets\n",
    "   - Training time overhead is acceptable for improved accuracy\n",
    "\n",
    "### Recommendations:\n",
    "- Use **CBR + COCOMO + XGBoost** ensemble for production systems\n",
    "- Apply **median combination** for robustness\n",
    "- Consider **hyperparameter tuning** for further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "print(f\"Figures saved to: ../reports/figures/\")\n",
    "print(\"\\nKey files generated:\")\n",
    "print(\"  - final_comparison.png\")\n",
    "print(\"  - training_time.png\")\n",
    "print(\"  - metrics_heatmap.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
