{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "This notebook demonstrates data preprocessing techniques for software effort estimation.\n",
    "\n",
    "## Contents\n",
    "1. Data loading\n",
    "2. Handling missing values\n",
    "3. Feature scaling\n",
    "4. Feature engineering\n",
    "5. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.preprocessor import DataPreprocessor\n",
    "from src.data.feature_engineering import FeatureEngineer, engineer_features\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader('cocomo81')\n",
    "df = loader.load_raw_data()\n",
    "\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate missing value handling\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Create artificial missing values for demonstration\n",
    "df_with_missing = df.copy()\n",
    "df_with_missing.iloc[0, 0] = np.nan\n",
    "df_with_missing.iloc[5, 3] = np.nan\n",
    "\n",
    "print(\"Before handling:\")\n",
    "print(df_with_missing.isnull().sum().sum(), \"missing values\")\n",
    "\n",
    "# Handle with mean imputation\n",
    "df_clean = preprocessor.handle_missing_values(df_with_missing, strategy='mean')\n",
    "\n",
    "print(\"\\nAfter handling:\")\n",
    "print(df_clean.isnull().sum().sum(), \"missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and target\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "print(f\"Original X statistics:\")\n",
    "print(f\"  Mean: {X.mean():.4f}\")\n",
    "print(f\"  Std: {X.std():.4f}\")\n",
    "print(f\"  Min: {X.min():.4f}\")\n",
    "print(f\"  Max: {X.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaling\n",
    "preprocessor = DataPreprocessor()\n",
    "X_standard = preprocessor.scale_features(X, method='standard')\n",
    "\n",
    "print(f\"\\nAfter Standard Scaling:\")\n",
    "print(f\"  Mean: {X_standard.mean():.4f} (should be ~0)\")\n",
    "print(f\"  Std: {X_standard.std():.4f} (should be ~1)\")\n",
    "print(f\"  Min: {X_standard.min():.4f}\")\n",
    "print(f\"  Max: {X_standard.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaling\n",
    "preprocessor2 = DataPreprocessor()\n",
    "X_minmax = preprocessor2.scale_features(X, method='minmax')\n",
    "\n",
    "print(f\"\\nAfter MinMax Scaling:\")\n",
    "print(f\"  Min: {X_minmax.min():.4f} (should be 0)\")\n",
    "print(f\"  Max: {X_minmax.max():.4f} (should be 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling effect\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].boxplot(X[:, :5])\n",
    "axes[0].set_title('Original Features (first 5)')\n",
    "axes[0].set_xticklabels(['rely', 'data', 'cplx', 'time', 'stor'])\n",
    "\n",
    "axes[1].boxplot(X_standard[:, :5])\n",
    "axes[1].set_title('Standard Scaled Features (first 5)')\n",
    "axes[1].set_xticklabels(['rely', 'data', 'cplx', 'time', 'stor'])\n",
    "\n",
    "axes[2].boxplot(X_minmax[:, :5])\n",
    "axes[2].set_title('MinMax Scaled Features (first 5)')\n",
    "axes[2].set_xticklabels(['rely', 'data', 'cplx', 'time', 'stor'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/scaling_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = FeatureEngineer()\n",
    "\n",
    "# Create EAF feature\n",
    "X_eaf = engineer.create_eaf_feature(X)\n",
    "print(f\"After adding EAF feature: {X_eaf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create size-derived features\n",
    "X_size = engineer.create_size_derived_features(X)\n",
    "print(f\"After adding size features: {X_size.shape}\")\n",
    "print(\"Added: log_loc, sqrt_loc, loc_squared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_names = loader.get_feature_names()\n",
    "importance = engineer.get_feature_importance(X, y, feature_names)\n",
    "print(\"\\nFeature Importance (Top 10):\")\n",
    "print(importance[['Feature', 'F_Score', 'MI_Score', 'Correlation', 'Avg_Rank']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "importance_sorted = importance.sort_values('F_Score', ascending=True)\n",
    "plt.barh(importance_sorted['Feature'], importance_sorted['F_Score'])\n",
    "plt.xlabel('F-Score')\n",
    "plt.title('Feature Importance (F-Score)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete preprocessing pipeline\n",
    "preprocessor = DataPreprocessor()\n",
    "X, y = preprocessor.preprocess_pipeline(df, scale=True)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target distribution in splits\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(y_train, bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'Training Set (n={len(y_train)})')\n",
    "axes[0].set_xlabel('Effort')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(y_test, bins=15, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title(f'Test Set (n={len(y_test)})')\n",
    "axes[1].set_xlabel('Effort')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Preprocessing Steps Applied:\n",
    "1. **Missing Value Handling**: Mean imputation\n",
    "2. **Feature Scaling**: StandardScaler (zero mean, unit variance)\n",
    "3. **Feature Engineering**: EAF calculation, size-derived features\n",
    "4. **Train-Test Split**: 80-20 split with random state for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
