{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlesP08hXCY8",
        "outputId": "f5b974b6-80e1-4d2c-c4bc-62cf4929243e"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split,LeaveOneOut\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "df = pd.read_csv('/content/nasa93dataset.csv')\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "delta = 1e-4\n",
        "\n",
        "# Similarity functions\n",
        "def similarity_euclidean(x1, x2, w):\n",
        "    return np.sqrt(np.sum(w * (x1 - x2) ** 2) + delta)\n",
        "\n",
        "def similarity_manhattan(x1, x2, w):\n",
        "    return np.sum(w * np.abs(x1 - x2)) + delta\n",
        "\n",
        "def similarity_maximum_distance(x1, x2, w):\n",
        "    return np.max(w * np.abs(x1 - x2))\n",
        "\n",
        "def similarity_mahalanobis(x1, x2, w, cov_inv):\n",
        "    diff = x1 - x2\n",
        "    return np.sqrt(diff.T @ cov_inv @ diff + delta)\n",
        "\n",
        "def similarity_akritean(x1, x2, w, omega1, omega2):\n",
        "    return omega1 * similarity_euclidean(x1, x2, w) + omega2 * similarity_manhattan(x1, x2, w)\n",
        "\n",
        "def inverse_weighted_mean(neighbor_efforts, similarities):\n",
        "    similarities = 1 / (similarities + 1e-9)\n",
        "    return np.sum((similarities / np.sum(similarities)) * neighbor_efforts)\n",
        "\n",
        "\n",
        "# Optimization function\n",
        "def optimise_parameters(X_train, y_train, X_val, y_val):\n",
        "    cov_matrix = np.cov(X_train, rowvar=False) + np.eye(X_train.shape[1]) * delta\n",
        "    cov_inv = np.linalg.inv(cov_matrix)\n",
        "\n",
        "    def objective(trial):\n",
        "        # Suggest feature and similarity weights\n",
        "        feature_weights = np.array([trial.suggest_float(f'fw_{i}', 0, 1) for i in range(X.shape[1])])\n",
        "        sim_weights = np.array([trial.suggest_float(f'sw_{i}', 0, 1) for i in range(5)])\n",
        "\n",
        "        # Ensure the sum of weights is 1\n",
        "        feature_weights /= feature_weights.sum()\n",
        "        sim_weights /= sim_weights.sum()\n",
        "\n",
        "        # Number of nearest neighbors\n",
        "        k = min(trial.suggest_int('k', 3, 20), len(X_train))\n",
        "\n",
        "        # Akritean similarity weights\n",
        "        omega1 = trial.suggest_float('omega1', 0, 1)\n",
        "        omega2 = 1 - omega1\n",
        "\n",
        "        # Solution method\n",
        "        solution_func = trial.suggest_categorical('solution', ['mean', 'median', 'inverse_mean'])\n",
        "\n",
        "        # Define similarity functions\n",
        "        similarity_functions = [\n",
        "            similarity_euclidean,\n",
        "            similarity_manhattan,\n",
        "            similarity_maximum_distance,\n",
        "            lambda x1, x2, w: similarity_mahalanobis(x1, x2, w, cov_inv),\n",
        "            lambda x1, x2, w: similarity_akritean(x1, x2, w, omega1, omega2)\n",
        "        ]\n",
        "\n",
        "        # Apply feature weights\n",
        "        weighted_X_train = X_train * feature_weights\n",
        "        weighted_X_val = X_val * feature_weights\n",
        "\n",
        "        predictions = []\n",
        "        for x in weighted_X_val:\n",
        "            effort_predictions = []\n",
        "\n",
        "            for sim_fn in similarity_functions:\n",
        "                similarities = np.array([sim_fn(x, train_row, feature_weights) for train_row in weighted_X_train])\n",
        "                neighbor_indices = np.argsort(similarities)[:k]\n",
        "                neighbor_efforts = y_train[neighbor_indices]\n",
        "\n",
        "                if solution_func == 'mean':\n",
        "                    pred = np.mean(neighbor_efforts)\n",
        "                elif solution_func == 'median':\n",
        "                    pred = np.median(neighbor_efforts)\n",
        "                else:\n",
        "                    pred = inverse_weighted_mean(neighbor_efforts, similarities[neighbor_indices])\n",
        "\n",
        "                effort_predictions.append(pred)\n",
        "\n",
        "            final_effort = np.sum(sim_weights * np.array(effort_predictions))\n",
        "            predictions.append(final_effort)\n",
        "\n",
        "        mae = mean_absolute_error(y_val, predictions)\n",
        "        print(f\"Trial {trial.number}: MAE = {mae}\")\n",
        "        return mae\n",
        "\n",
        "    # Optimize with parallel execution and early stopping\n",
        "    study = optuna.create_study(\n",
        "        direction='minimize',\n",
        "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
        "    )\n",
        "    # Changed import for the callback\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    # Display best result\n",
        "    best_trial = study.best_trial\n",
        "    print(\"\\nBest Trial:\")\n",
        "    print(f\"  MAE: {best_trial.value}\")\n",
        "    print(\"  Parameters:\")\n",
        "    for key, value in best_trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    return best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_495pUZoU2qz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_mmre(actuals, predictions):\n",
        "    actuals = np.array(actuals)\n",
        "    predictions = np.array(predictions)\n",
        "    relative_errors = np.abs((actuals - predictions) / np.where(actuals == 0, 1e-10, actuals))\n",
        "    return np.mean(relative_errors)\n",
        "\n",
        "def calculate_bmmre(actuals, all_predictions):\n",
        "    actuals = np.array(actuals)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    relative_errors = [np.abs((actuals - pred) / np.where(actuals == 0, 1e-10, actuals)) for pred in all_predictions]\n",
        "    min_relative_errors = np.min(relative_errors, axis=0)\n",
        "    return np.mean(min_relative_errors)\n",
        "\n",
        "\n",
        "def calculate_mdmre(y_actual, y_pred):\n",
        "    if len(y_actual) != len(y_pred):\n",
        "        raise ValueError(\"y_actual and y_pred must have the same length.\")\n",
        "\n",
        "    # Calculate MRE for each prediction\n",
        "    mre = np.abs((np.array(y_pred) - np.array(y_actual)) / np.array(y_actual))\n",
        "\n",
        "    # Calculate the median of MRE\n",
        "    mdmre = np.median(mre)\n",
        "\n",
        "    return mdmre\n",
        "\n",
        "\n",
        "def pred_25(y_true, y_pred):\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    percentage_error = np.abs(y_true - y_pred) / y_true\n",
        "    within_25_percent = np.sum(percentage_error <= 0.25)\n",
        "\n",
        "    return within_25_percent / len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0gjVgKm0Psz",
        "outputId": "67a365b4-a49c-4b67-e651-7d028d6dcd4a"
      },
      "outputs": [],
      "source": [
        "loo = LeaveOneOut()\n",
        "errors = []\n",
        "final_predictions = []\n",
        "predictions = []\n",
        "actuals = []\n",
        "it = 1\n",
        "\n",
        "for train_val_index, test_index in loo.split(X):\n",
        "    print(f\"Iteration -{it}\")\n",
        "    it = it+1\n",
        "    # Leave-One-Out: 1 row as test, the rest for training + validation\n",
        "    X_train_val, X_test = X[train_val_index], X[test_index]\n",
        "    y_train_val, y_test = y[train_val_index], y[test_index]\n",
        "\n",
        "    # Split train_val into 70% Training, 30% Validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=42)\n",
        "    best_params = optimise_parameters(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Final Model Evaluation using Test Set\n",
        "    feature_weights = np.array([best_params[f'fw_{i}'] for i in range(X.shape[1])])\n",
        "    k = best_params['k']\n",
        "    sim_weights = np.array([best_params[f'sw_{i}'] for i in range(5)])\n",
        "    omega1 = best_params['omega1']\n",
        "    omega2 = 1 - omega1\n",
        "    solution_func = best_params['solution']\n",
        "\n",
        "    # Normalize weights\n",
        "    feature_weights /= feature_weights.sum()\n",
        "    sim_weights /= sim_weights.sum()\n",
        "\n",
        "    cov_matrix = np.cov(X_train, rowvar=False) + np.eye(X_train.shape[1]) * 1e-3  # Regularization\n",
        "    cov_inv = np.linalg.inv(cov_matrix)\n",
        "\n",
        "    similarity_functions = [\n",
        "        similarity_euclidean,\n",
        "        similarity_manhattan,\n",
        "        similarity_maximum_distance,\n",
        "        lambda x1, x2, w: similarity_mahalanobis(x1, x2, w, cov_inv),\n",
        "        lambda x1, x2, w: similarity_akritean(x1, x2, w, omega1, omega2)\n",
        "    ]\n",
        "\n",
        "    # Apply feature weights to training data\n",
        "    weighted_X_train_val = X_train_val * feature_weights\n",
        "    weighted_X_test = X_test[0] * feature_weights  # Single test row\n",
        "\n",
        "    effort_predictions_test = []\n",
        "    for sim_fn in similarity_functions:\n",
        "        similarities = np.array([sim_fn(weighted_X_test, train_row, feature_weights) for train_row in weighted_X_train_val])\n",
        "        neighbor_indices = np.argsort(similarities)[:k]\n",
        "        neighbor_efforts = y_train_val[neighbor_indices]\n",
        "\n",
        "        if solution_func == 'mean':\n",
        "            pred = np.mean(neighbor_efforts)\n",
        "        elif solution_func == 'median':\n",
        "            pred = np.median(neighbor_efforts)\n",
        "        elif solution_func == 'inverse_mean':\n",
        "            pred = inverse_weighted_mean(neighbor_efforts, similarities[neighbor_indices])\n",
        "\n",
        "        effort_predictions_test.append(pred)\n",
        "\n",
        "    final_effort_test = np.sum(sim_weights * np.array(effort_predictions_test))\n",
        "\n",
        "    # Compute MAE for test sample\n",
        "    mae_test = abs(y_test[0] - final_effort_test)\n",
        "    predictions.append(final_effort_test)\n",
        "    actuals.append(y_test[0])\n",
        "    errors.append(mae_test)\n",
        "\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE) using LOOCV: {np.mean(errors)}\")\n",
        "print(errors)\n",
        "print(np.mean(errors))\n",
        "print(actuals)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EaB8LkPLCcG2",
        "outputId": "5db4c4b3-663a-4c0c-a6f7-11124748eba2"
      },
      "outputs": [],
      "source": [
        "print(errors)\n",
        "print(np.mean(errors))\n",
        "print(actuals)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ejQoHcYDdd0",
        "outputId": "66667c73-fa61-4c7b-9ce9-c49ddb8e4243"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kfold = KFold(n_splits=4, shuffle=True, random_state=42)\n",
        "errors = []\n",
        "final_predictions = []\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "for train_val_index, test_index in kfold.split(X):\n",
        "    # Leave-One-Out: 1 row as test, the rest for training + validation\n",
        "    X_train_val, X_test = X[train_val_index], X[test_index]\n",
        "    y_train_val, y_test = y[train_val_index], y[test_index]\n",
        "\n",
        "    # Split train_val into 70% Training, 30% Validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=42)\n",
        "    best_params = optimise_parameters(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Final Model Evaluation using Test Set\n",
        "    feature_weights = np.array([best_params[f'fw_{i}'] for i in range(X.shape[1])])\n",
        "    k = best_params['k']\n",
        "    sim_weights = np.array([best_params[f'sw_{i}'] for i in range(5)])\n",
        "    omega1 = best_params['omega1']\n",
        "    omega2 = 1 - omega1\n",
        "    solution_func = best_params['solution']\n",
        "\n",
        "    # Normalize weights\n",
        "    feature_weights /= feature_weights.sum()\n",
        "    sim_weights /= sim_weights.sum()\n",
        "\n",
        "    cov_matrix = np.cov(X_train, rowvar=False) + np.eye(X_train.shape[1]) * delta\n",
        "    cov_inv = np.linalg.inv(cov_matrix)\n",
        "\n",
        "    similarity_functions = [\n",
        "        similarity_euclidean,\n",
        "        similarity_manhattan,\n",
        "        similarity_maximum_distance,\n",
        "        lambda x1, x2, w: similarity_mahalanobis(x1, x2, w, cov_inv),\n",
        "        lambda x1, x2, w: similarity_akritean(x1, x2, w, omega1, omega2)\n",
        "    ]\n",
        "\n",
        "    # Apply feature weights to training data\n",
        "    weighted_X_train_val = X_train_val * feature_weights\n",
        "    weighted_X_test = X_test[0] * feature_weights  # Single test row\n",
        "\n",
        "    effort_predictions_test = []\n",
        "    for sim_fn in similarity_functions:\n",
        "        similarities = np.array([sim_fn(weighted_X_test, train_row, feature_weights) for train_row in weighted_X_train_val])\n",
        "        neighbor_indices = np.argsort(similarities)[:k]\n",
        "        neighbor_efforts = y_train_val[neighbor_indices]\n",
        "\n",
        "        if solution_func == 'mean':\n",
        "            pred = np.mean(neighbor_efforts)\n",
        "        elif solution_func == 'median':\n",
        "            pred = np.median(neighbor_efforts)\n",
        "        elif solution_func == 'inverse_mean':\n",
        "            pred = inverse_weighted_mean(neighbor_efforts, similarities[neighbor_indices])\n",
        "\n",
        "        effort_predictions_test.append(pred)\n",
        "\n",
        "    final_effort_test = np.sum(sim_weights * np.array(effort_predictions_test))\n",
        "\n",
        "    # Compute MAE for test sample\n",
        "    mae_test = np.mean(abs(y_test - final_effort_test))\n",
        "    predictions.append(final_effort_test)\n",
        "    actuals.append(y_test[0])\n",
        "    errors.append(mae_test)\n",
        "\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE) using LOOCV: {np.mean(errors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g0o35hVMD_on",
        "outputId": "b9082ec2-dbab-4a66-99a1-e450d339cdb3"
      },
      "outputs": [],
      "source": [
        "print(errors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
